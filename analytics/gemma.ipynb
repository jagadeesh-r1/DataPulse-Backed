{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaggu/cewit/hack-cewit2024/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "modelb16 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model4bit = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Suppose you are walking on a beach and you see a turtle. What do you do next? You probably look at the turtle, and then you look at the turtle again. You probably look at the turtle for a while, and then you look at something else. You probably look at the turtle for a while, and then you look at something else. You probably look at the turtle for a while, and then you look at something else. You probably look at the turtle for a while, and then you look at something else. You probably look at the turtle for a while, and then\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Suppose you are walking on a beach and you see a turtle. What do you do next?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Please respond as DATAPULSE! an AI assistant. User: How are you. Answer in 100 words or less. DATAPULSE: Good morning, and thank you for your question.\n",
      "\n",
      "<h2>How do you say hello in Japanese?</h2>\n",
      "\n",
      "Konban wa Kimi ni Iwareta to, Kimi wa Konnani tsutaeta desu.\n",
      "\n",
      "<h2>How do you say hello in Japanese?</h2>\n",
      "\n",
      "Konban desu.\n",
      "\n",
      "<h2>How do you say hello in Japanese?</h2>\n",
      "\n",
      "Konban desu.\n",
      "\n",
      "<h2>How do you say hello in Japanese?</h2>\n",
      "\n",
      "Konban desu.\n",
      "\n",
      "<h2>How do you say hello in Japanese?</h2>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Please respond as DATAPULSE! an AI assistant. User: How are you. Answer in 100 words or less. DATAPULSE:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model4bit.generate(**input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos>Please respond as DATAPULSE! an AI assistant. Hi. Answer in 100 words or less.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. What is your name?\n",
      "2. What is your email address?\n",
      "3. What is your phone number?\n",
      "4. What is your date of birth?\n",
      "5. What is your gender?\n",
      "6. What is your address?\n",
      "7. What is your ZIP code?\n",
      "8. What is your mother's maiden name?\n",
      "9. What is your father's name?\n",
      "10. What is your date of death?\n",
      "11. What is our phone#\n",
      "12. When will you graduate from High\n",
      "13. How old are you?\n",
      "14. When did your high school graduate?\n",
      "15. Who's your dream date?\n",
      "16. What's your dream car?\n",
      "17. What'sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n"
     ]
    }
   ],
   "source": [
    "if not tokenizer.decode(outputs[0]).endswith('<eos>'):\n",
    "    input_ids = tokenizer.encode(tokenizer.decode(outputs[0]), return_tensors='pt')\n",
    "    output = model4bit.generate(input_ids, max_new_tokens=100, do_sample=True)\n",
    "    generated_text = tokenizer.decode(output[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
